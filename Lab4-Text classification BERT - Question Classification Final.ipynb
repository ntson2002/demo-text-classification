{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0809338f-0a30-4244-a4da-03cad9a56f72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.57.1)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (4.4.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: fsspec[http]<=2025.10.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2025.10.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (2023.5.7)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (3.4)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets) (1.1.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (1.11.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.24.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.9.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.6.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (23.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.36.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.10.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
      "Requirement already satisfied: triton==3.5.0 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (1.26.15)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.0.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers[torch] in /opt/conda/lib/python3.10/site-packages (4.57.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2025.11.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (1.24.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.22.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.36.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.32.5)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.6.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (6.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (3.9.0)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (1.11.0)\n",
      "Requirement already satisfied: torch>=2.2 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.9.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.0->transformers[torch]) (5.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (4.15.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (2025.10.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/conda/lib/python3.10/site-packages (from torch>=2.2->transformers[torch]) (3.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/conda/lib/python3.10/site-packages (from torch>=2.2->transformers[torch]) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/conda/lib/python3.10/site-packages (from torch>=2.2->transformers[torch]) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/conda/lib/python3.10/site-packages (from torch>=2.2->transformers[torch]) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/conda/lib/python3.10/site-packages (from torch>=2.2->transformers[torch]) (1.13.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/conda/lib/python3.10/site-packages (from torch>=2.2->transformers[torch]) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/conda/lib/python3.10/site-packages (from torch>=2.2->transformers[torch]) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/conda/lib/python3.10/site-packages (from torch>=2.2->transformers[torch]) (0.7.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/conda/lib/python3.10/site-packages (from torch>=2.2->transformers[torch]) (12.8.4.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.10/site-packages (from torch>=2.2->transformers[torch]) (1.14.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/conda/lib/python3.10/site-packages (from torch>=2.2->transformers[torch]) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/conda/lib/python3.10/site-packages (from torch>=2.2->transformers[torch]) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /opt/conda/lib/python3.10/site-packages (from torch>=2.2->transformers[torch]) (2.27.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/conda/lib/python3.10/site-packages (from torch>=2.2->transformers[torch]) (12.8.93)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/conda/lib/python3.10/site-packages (from torch>=2.2->transformers[torch]) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /opt/conda/lib/python3.10/site-packages (from torch>=2.2->transformers[torch]) (3.3.20)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=2.2->transformers[torch]) (3.1.2)\n",
      "Requirement already satisfied: triton==3.5.0 in /opt/conda/lib/python3.10/site-packages (from torch>=2.2->transformers[torch]) (3.5.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/conda/lib/python3.10/site-packages (from torch>=2.2->transformers[torch]) (11.7.3.90)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.15)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (2023.5.7)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.2->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=2.2->transformers[torch]) (2.1.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Transformers installation\n",
    "! pip install transformers datasets\n",
    "! pip install accelerate -U\n",
    "! pip install transformers[torch]\n",
    "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
    "# ! pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38791517-5b25-444b-9456-a837c23b120e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.6)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.4.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.3.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.36.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (23.0)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.4.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.18)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.24.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2025.10.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.28.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (22.0.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.5.7)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (5.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.11.0)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets>=2.0.0->evaluate) (1.1.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.7.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install evaluate\n",
    "! pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e8ed62-12ee-4ad5-a89c-497d72cf7287",
   "metadata": {},
   "source": [
    "# Convert TREC data to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0605ca8c-62d3-4ca7-8745-ad1fbb23bac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import codecs\n",
    "def _generate_examples(filepath):\n",
    "    label2id = {'ABBR':0, 'DESC':1, 'ENTY':2, 'HUM':3, 'LOC':4, 'NUM':5}\n",
    "    id2label = {0:'ABBR', 1:'DESC', 2:'ENTY', 3:'HUM', 4:'LOC', 5:'NUM'}\n",
    "    examples = []\n",
    "    with codecs.open(filepath, \"rb\") as f:\n",
    "        for id_, row in enumerate(f):\n",
    "            # One non-ASCII byte: sisterBADBYTEcity. We replace it with a space\n",
    "            label, _, text = row.replace(b\"\\xf0\",\n",
    "                                         b\" \").strip().decode().partition(\" \")\n",
    "            coarse_label, _, fine_label = label.partition(\":\")\n",
    "            examples.append({\n",
    "                'id': id_, \n",
    "                # \"label-fine\": fine_label,\n",
    "                \"text\": text,\n",
    "                \"label\": label2id[coarse_label],\n",
    "                \"label-coarse\": coarse_label,                \n",
    "            })\n",
    "    return examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9817cb92-f380-433a-9c49-8a823d689854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import codecs \n",
    "import unicodedata\n",
    "def write_json(o, saved):    \n",
    "    data = unicodedata.normalize('NFC', json.dumps(o, indent=4, ensure_ascii=False))\n",
    "    with codecs.open(saved, mode=\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b9a50c8-a10d-4fe6-9b18-bc6820c891b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = _generate_examples(\"data/trec/train_5500.label\")\n",
    "test = _generate_examples(\"data/trec/TREC_10.label\")\n",
    "\n",
    "write_json(train, \"data/trec/trec_train.json\")\n",
    "write_json(test, \"data/trec/trec_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0c5c9d2-029b-4546-9ef3-eb715e4fc35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 5452 Test: 500\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train: {len(train)} Test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c2cd953-b198-4978-8e1e-2249d4d97d93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'Could not load this library: /opt/conda/lib/python3.10/site-packages/torchvision/image.so'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS\n",
    "# =============================================================================\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import warnings\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "# Environment variables - Set BEFORE any imports that use them\n",
    "# os.environ['HTTP_PROXY'] = \"http://10.60.28.99:81\"\n",
    "# os.environ['HTTPS_PROXY'] = \"http://10.60.28.99:81\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # Fix tokenizers parallelism warning\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # Optional: disable oneDNN if not needed\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "OUTPUT_DIR = \"trec_model\"\n",
    "NUM_LABELS = 6  # Updated to 6 labels\n",
    "\n",
    "# Updated label mappings for 6 classes\n",
    "id2label = {0: 'ABBR', 1: 'DESC', 2: 'ENTY', 3: 'HUM', 4: 'LOC', 5: 'NUM'}\n",
    "label2id = {'ABBR': 0, 'DESC': 1, 'ENTY': 2, 'HUM': 3, 'LOC': 4, 'NUM': 5}\n",
    "\n",
    "# Training hyperparameters\n",
    "LEARNING_RATE = 2e-5\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 4\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DATA LOADING FUNCTIONS\n",
    "# =============================================================================\n",
    "def load_from_json(filename):\n",
    "    \"\"\"Load dataset from a JSON file and convert to Hugging Face Dataset format\"\"\"\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Extract texts and labels\n",
    "    texts = [item['text'] for item in data]\n",
    "    labels = [item['label'] for item in data]\n",
    "    \n",
    "    # Convert to Hugging Face Dataset\n",
    "    dataset = Dataset.from_dict({\n",
    "        'text': texts,\n",
    "        'label': labels\n",
    "    })\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PREPROCESSING FUNCTIONS\n",
    "# =============================================================================\n",
    "# Note: These functions need to be defined at module level for proper serialization\n",
    "def create_preprocess_function(tokenizer):\n",
    "    \"\"\"Create a preprocessing function with tokenizer\"\"\"\n",
    "    def preprocess_function(examples):\n",
    "        \"\"\"Tokenize text examples\"\"\"\n",
    "        return tokenizer(examples[\"text\"], truncation=True, padding=False)\n",
    "    return preprocess_function\n",
    "\n",
    "\n",
    "def create_compute_metrics(accuracy_metric):\n",
    "    \"\"\"Create a compute metrics function with accuracy metric\"\"\"\n",
    "    def compute_metrics(eval_pred):\n",
    "        \"\"\"Compute accuracy metrics for evaluation\"\"\"\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    return compute_metrics\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf2d494-fde0-4682-b301-8d85f4c1a61d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48b48dc5-c51e-49e9-993d-98fa9996a198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading from JSON files...\n",
      "Train dataset size: 5452\n",
      "Test dataset size: 500\n",
      "\n",
      "Initializing tokenizer...\n",
      "Tokenizing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60dfcb3edb924fd1a1ac2bd732f3f20e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing training data:   0%|          | 0/5452 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7843840c2c014cd1a8de062f9fccc8bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing test data:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing data collator and metrics...\n",
      "\n",
      "Initializing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing trainer...\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1364' max='1364' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1364/1364 00:34, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.215661</td>\n",
       "      <td>0.942000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.497900</td>\n",
       "      <td>0.148410</td>\n",
       "      <td>0.962000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.113200</td>\n",
       "      <td>0.123353</td>\n",
       "      <td>0.974000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.113200</td>\n",
       "      <td>0.134205</td>\n",
       "      <td>0.972000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model...\n",
      "\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results: {'eval_loss': 0.12335308641195297, 'eval_accuracy': 0.974, 'eval_runtime': 0.1969, 'eval_samples_per_second': 2539.349, 'eval_steps_per_second': 162.518, 'epoch': 4.0}\n",
      "\n",
      "Training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    # Load datasets\n",
    "    print(\"\\nLoading from JSON files...\")\n",
    "    train_dataset = load_from_json('data/trec/trec_train.json')\n",
    "    test_dataset = load_from_json('data/trec/trec_test.json')\n",
    "    print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    print(\"\\nInitializing tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    # Create preprocessing function\n",
    "    preprocess_function = create_preprocess_function(tokenizer)\n",
    "    \n",
    "    # Tokenize datasets\n",
    "    print(\"Tokenizing datasets...\")\n",
    "    tokenized_train_imdb = train_dataset.map(\n",
    "        preprocess_function, \n",
    "        batched=True,\n",
    "        desc=\"Tokenizing training data\"\n",
    "    )\n",
    "    tokenized_test_imdb = test_dataset.map(\n",
    "        preprocess_function, \n",
    "        batched=True,\n",
    "        desc=\"Tokenizing test data\"\n",
    "    )\n",
    "    \n",
    "    # Initialize data collator and metrics\n",
    "    print(\"\\nInitializing data collator and metrics...\")\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    accuracy = evaluate.load(\"accuracy\")\n",
    "    compute_metrics = create_compute_metrics(accuracy)\n",
    "    \n",
    "    # Initialize model\n",
    "    print(\"\\nInitializing model...\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=NUM_LABELS,  # Updated for 6 labels\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "    \n",
    "    # Configure training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=False,\n",
    "        report_to=\"none\",  # Disable all reporting including wandb\n",
    "        logging_steps=500,\n",
    "        logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "        dataloader_num_workers=0,  # Avoid multiprocessing issues\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer - use processing_class instead of tokenizer\n",
    "    print(\"\\nInitializing trainer...\")\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_imdb,\n",
    "        eval_dataset=tokenized_test_imdb,\n",
    "        processing_class=tokenizer,  # Use processing_class instead of tokenizer\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    # Start training\n",
    "    print(\"\\nStarting training...\")\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    # Save the final model\n",
    "    print(\"\\nSaving model...\")\n",
    "    trainer.save_model()\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_results = trainer.evaluate(tokenized_test_imdb)\n",
    "    print(f\"Test results: {test_results}\")\n",
    "    \n",
    "    print(\"\\nTraining completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0a088e4-34de-4d44-bdee-deaabe58efb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.12335308641195297,\n",
       " 'eval_accuracy': 0.974,\n",
       " 'eval_runtime': 0.1969,\n",
       " 'eval_samples_per_second': 2539.349,\n",
       " 'eval_steps_per_second': 162.518,\n",
       " 'epoch': 4.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cdc6893-4e48-41fc-8782-c3e767a7d90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint-1023  checkpoint-682     special_tokens_map.json  training_args.bin\n",
      "checkpoint-1364  config.json\t    tokenizer.json\t     vocab.txt\n",
      "checkpoint-341\t model.safetensors  tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "! ls trec_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5c157d-9335-4848-8bfa-240e3745b204",
   "metadata": {},
   "source": [
    "# Load and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9902983a-f12f-4edb-96be-bf00b31ce78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading from trec_test.json...\n",
      "Test dataset size: 500\n",
      "\n",
      "Initializing tokenizer...\n",
      "Tokenizing test data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2fd8a6dba2944b29143c55acc5cac53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing test data:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model from checkpoint...\n",
      "\n",
      "Performing inference on the test dataset...\n",
      "Accuracy: 97.20%\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS\n",
    "# =============================================================================\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score  # For manual accuracy calculation\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "# Environment variables - Set BEFORE any imports that use them\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "OUTPUT_DIR = \"trec_model\"\n",
    "CHECKPOINT_PATH = os.path.join(OUTPUT_DIR, \"checkpoint-1364\")  # Path to the checkpoint\n",
    "NUM_LABELS = 6  # Number of classes\n",
    "\n",
    "# Updated label mappings for 6 classes\n",
    "id2label = {0: 'ABBR', 1: 'DESC', 2: 'ENTY', 3: 'HUM', 4: 'LOC', 5: 'NUM'}\n",
    "label2id = {'ABBR': 0, 'DESC': 1, 'ENTY': 2, 'HUM': 3, 'LOC': 4, 'NUM': 5}\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"Load the model, perform inference and calculate accuracy\"\"\"\n",
    "    # Load the test dataset (trec_test.json)\n",
    "    print(\"\\nLoading from trec_test.json...\")\n",
    "    test_dataset = load_from_json('data/trec/trec_test.json')\n",
    "    print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    print(\"\\nInitializing tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    # Tokenize the test dataset\n",
    "    print(\"Tokenizing test data...\")\n",
    "    def preprocess_function(examples):\n",
    "        \"\"\"Tokenize text examples\"\"\"\n",
    "        return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=128)\n",
    "    \n",
    "    tokenized_test_data = test_dataset.map(\n",
    "        preprocess_function, \n",
    "        batched=True,\n",
    "        desc=\"Tokenizing test data\"\n",
    "    )\n",
    "\n",
    "    # Load the model from checkpoint\n",
    "    print(\"\\nLoading model from checkpoint...\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        CHECKPOINT_PATH,\n",
    "        num_labels=NUM_LABELS,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "    \n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Define a custom collate function for DataLoader\n",
    "    def collate_fn(batch):\n",
    "        \"\"\"Collate function for DataLoader\"\"\"\n",
    "        return {\n",
    "            'input_ids': torch.stack([torch.tensor(item['input_ids']) for item in batch]),\n",
    "            'attention_mask': torch.stack([torch.tensor(item['attention_mask']) for item in batch]),\n",
    "            'labels': torch.tensor([item['label'] for item in batch])\n",
    "        }\n",
    "\n",
    "    # Use DataLoader for batching\n",
    "    from torch.utils.data import DataLoader\n",
    "    test_dataloader = DataLoader(tokenized_test_data, batch_size=16, collate_fn=collate_fn)\n",
    "\n",
    "    # Run inference on the test dataset\n",
    "    print(\"\\nPerforming inference on the test dataset...\")\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        # Move data to the appropriate device (GPU/CPU)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "        \n",
    "        predictions.extend(preds)\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Compute accuracy manually\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d4e346-2f83-4b83-954f-bfb050fa77a2",
   "metadata": {},
   "source": [
    "# Load and inference using pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af6c8198-c08b-4f1d-baf0-23f493e4d2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_classifier():\n",
    "    # Single example \n",
    "    from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "    import torch\n",
    "\n",
    "    # Define the model and checkpoint paths\n",
    "    MODEL_PATH = \"trec_model/checkpoint-682\"  # Adjust path to your model checkpoint\n",
    "\n",
    "    # Load the pipeline for multi-class classification (6 classes)\n",
    "    classifier = pipeline(\n",
    "        \"text-classification\",\n",
    "        model=MODEL_PATH,\n",
    "        tokenizer=MODEL_PATH,\n",
    "        device=0 if torch.cuda.is_available() else -1  # Use GPU if available\n",
    "    )\n",
    "    return classifier\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer():\n",
    "    MODEL_PATH = \"trec_model/checkpoint-682\"  # Adjust path to your model checkpoint\n",
    "    # Optionally, if you want to use the model and tokenizer directly:\n",
    "    # 1. Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1df6f00-f447-4cd2-a08d-8fb5131bb8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(TEXT, classifier):\n",
    "    # Classify the text\n",
    "    predictions = classifier(TEXT)\n",
    "\n",
    "    # Print the predictions\n",
    "    print(predictions)\n",
    "    \n",
    "def classify2(TEXT, model, tokenizer):\n",
    "    # 2. Prepare inputs\n",
    "    inputs = tokenizer(TEXT, return_tensors=\"pt\")\n",
    "\n",
    "    # 3. Run inference (with no gradient computation)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    # 4. Get the predicted class id and the label\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "    predicted_class = model.config.id2label[predicted_class_id]\n",
    "    print(logits)\n",
    "    print(f\"Predicted class: {predicted_class} (ID: {predicted_class_id})\")\n",
    "    \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def classify2(TEXT, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Classify text and print detailed scores for all classes\n",
    "    \n",
    "    Args:\n",
    "        TEXT: Input text to classify\n",
    "        model: Trained model\n",
    "        tokenizer: Tokenizer\n",
    "    \"\"\"\n",
    "    # 1. Prepare inputs\n",
    "    inputs = tokenizer(TEXT, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move to same device as model\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # 2. Run inference (with no gradient computation)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    \n",
    "    # 3. Convert logits to probabilities using softmax\n",
    "    probabilities = F.softmax(logits, dim=-1)\n",
    "    \n",
    "    # 4. Get the predicted class id and the label\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "    predicted_class = model.config.id2label[predicted_class_id]\n",
    "    confidence = probabilities[0][predicted_class_id].item()\n",
    "    \n",
    "    # 5. Print results\n",
    "    print(f\"\\nInput text: {TEXT}\")\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Predicted class: {predicted_class} (ID: {predicted_class_id})\")\n",
    "    print(f\"Confidence: {confidence:.4f} ({confidence*100:.2f}%)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(f\"\\nRaw logits:\")\n",
    "    print(logits)\n",
    "    \n",
    "    print(f\"\\nScores for all classes:\")\n",
    "    print(f\"{'Class':<15} {'Label':<12} {'Probability':<12} {'Percentage'}\")\n",
    "    print(f\"{'-'*60}\")\n",
    "    \n",
    "    # Sort by probability (descending)\n",
    "    probs_list = probabilities[0].cpu().numpy()\n",
    "    sorted_indices = probs_list.argsort()[::-1]\n",
    "    \n",
    "    for idx in sorted_indices:\n",
    "        class_label = model.config.id2label[idx]\n",
    "        prob = probs_list[idx]\n",
    "        marker = \"\" if idx == predicted_class_id else \" \"\n",
    "        print(f\"{marker} Class {idx:<7} {class_label:<12} {prob:<12.6f} {prob*100:>6.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'predicted_class': predicted_class,\n",
    "        'predicted_class_id': predicted_class_id,\n",
    "        'confidence': confidence,\n",
    "        'probabilities': probs_list,\n",
    "        'logits': logits[0].cpu().numpy()\n",
    "    }\n",
    "\n",
    "\n",
    "# Alternative: More compact version\n",
    "def classify2_compact(TEXT, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Compact version with scores\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(TEXT, return_tensors=\"pt\")\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    \n",
    "    probabilities = F.softmax(logits, dim=-1)[0]\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "    \n",
    "    print(f\"\\nText: {TEXT}\")\n",
    "    print(f\"\\nPrediction: {model.config.id2label[predicted_class_id]} ({probabilities[predicted_class_id]:.2%})\")\n",
    "    print(f\"\\nAll scores:\")\n",
    "    for idx, prob in enumerate(probabilities):\n",
    "        label = model.config.id2label[idx]\n",
    "        print(f\"  {label:10s}: {prob:.4f} ({prob:.2%})\")\n",
    "\n",
    "\n",
    "# Alternative: With visualization\n",
    "def classify2_visual(TEXT, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Version with visual bar chart\n",
    "    \"\"\"\n",
    "    import torch.nn.functional as F\n",
    "    \n",
    "    inputs = tokenizer(TEXT, return_tensors=\"pt\")\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    \n",
    "    probabilities = F.softmax(logits, dim=-1)[0]\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "    \n",
    "    print(f\"\\nText: {TEXT}\")\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Prediction: {model.config.id2label[predicted_class_id]}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Sort by probability\n",
    "    probs_sorted = sorted(\n",
    "        enumerate(probabilities.cpu().numpy()),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    for idx, prob in probs_sorted:\n",
    "        label = model.config.id2label[idx]\n",
    "        bar_length = int(prob * 50)  # Scale to 50 chars max\n",
    "        bar = '' * bar_length\n",
    "        marker = '' if idx == predicted_class_id else ' '\n",
    "        print(f\"{marker} {label:10s} {prob:6.2%} |{bar}\")\n",
    "    \n",
    "    return probabilities.cpu().numpy()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28cdf762-9dff-4fa3-8ede-9c509a8964db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'HUM', 'score': 0.995111882686615}]\n"
     ]
    }
   ],
   "source": [
    "classifier = load_classifier()\n",
    "classify(\"Who is Newton\", classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0a08693-f46f-4fc3-91e4-3dd0bba038e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NUM', 'score': 0.9757037162780762}, {'label': 'NUM', 'score': 0.9944137334823608}]\n"
     ]
    }
   ],
   "source": [
    "classify([\"The number is 5\", \n",
    "          \"How many students in the class\"], classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1c28545-cbcf-4dc9-a496-f8519cbac8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model_and_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df6aa891-c098-4e36-9aad-1f5f7307cb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input text: The number is 5\n",
      "\n",
      "============================================================\n",
      "Predicted class: NUM (ID: 5)\n",
      "Confidence: 0.9757 (97.57%)\n",
      "============================================================\n",
      "\n",
      "Raw logits:\n",
      "tensor([[-1.6028, -1.4694, -0.2186, -1.3284, -0.9530,  4.3271]])\n",
      "\n",
      "Scores for all classes:\n",
      "Class           Label        Probability  Percentage\n",
      "------------------------------------------------------------\n",
      " Class 5       NUM          0.975704      97.57%\n",
      "  Class 2       ENTY         0.010356       1.04%\n",
      "  Class 4       LOC          0.004969       0.50%\n",
      "  Class 3       HUM          0.003413       0.34%\n",
      "  Class 1       DESC         0.002965       0.30%\n",
      "  Class 0       ABBR         0.002594       0.26%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'predicted_class': 'NUM',\n",
       " 'predicted_class_id': 5,\n",
       " 'confidence': 0.9757035970687866,\n",
       " 'probabilities': array([0.00259426, 0.00296453, 0.0103557 , 0.00341332, 0.00496856,\n",
       "        0.9757036 ], dtype=float32),\n",
       " 'logits': array([-1.6027926 , -1.469377  , -0.21855755, -1.3284097 , -0.9529653 ,\n",
       "         4.3270645 ], dtype=float32)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify2(\"The number is 5\", model, tokenizer)\n",
    "# classify2_compact(\"The number is 5\", model, tokenizer)\n",
    "# classify2_visual(\"The number is 5\", model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60d6fc9e-2779-467b-a99b-e9869a47bde2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_test():\n",
    "    # Predict multi example\n",
    "    from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "    import json\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    # Define the model checkpoint path\n",
    "    MODEL_PATH = \"trec_model/checkpoint-1364\"  # Update to your checkpoint path\n",
    "\n",
    "    # Load the pipeline for multi-class classification (6 classes)\n",
    "    print(\"\\nLoading model and tokenizer...\")\n",
    "    classifier = pipeline(\n",
    "        \"text-classification\",\n",
    "        model=MODEL_PATH,\n",
    "        tokenizer=MODEL_PATH,\n",
    "        device=0 if torch.cuda.is_available() else -1  # Use GPU if available\n",
    "    )\n",
    "    dataset = load_from_json('data/trec/trec_test.json')\n",
    "\n",
    "    # Load texts and labels from trec_test.json\n",
    "    test_texts, true_labels = list(dataset[\"text\"]), list(dataset[\"label\"])\n",
    "\n",
    "    # Perform predictions using the pipeline\n",
    "    print(\"\\nClassifying text data from trec_test.json...\")\n",
    "\n",
    "    # Classify the texts\n",
    "    predictions = classifier(test_texts)\n",
    "\n",
    "    # Extract predicted labels (the 'label' field from the predictions)\n",
    "    predicted_labels = [prediction['label'] for prediction in predictions]\n",
    "\n",
    "    # Print out the predictions and compute accuracy\n",
    "    correct_predictions = sum([1 for true, pred in zip(true_labels, predicted_labels) if true == label2id[pred]])\n",
    "    total_predictions = len(true_labels)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    # Print the accuracy\n",
    "    print(f\"\\nAccuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Optionally, if you want\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8fdbe1f-f437-4a7b-8768-91403221c67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model and tokenizer...\n",
      "\n",
      "Classifying text data from trec_test.json...\n",
      "\n",
      "Accuracy: 97.20%\n"
     ]
    }
   ],
   "source": [
    "evaluate_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9d449d65-47ad-49ca-84b7-16b963be3e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38c832b-14d6-4e32-883f-352c3e292316",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7035808c-2f7f-4671-bdbc-f3a58d642b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac86713c-a8a1-49d8-827a-51efad938f56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937150b7-071e-421b-adac-1ee856d6f6cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
